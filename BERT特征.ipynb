{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95381fa6-ff56-40f5-9dd0-bd678c9a62f0",
   "metadata": {},
   "source": [
    "主要参考：https://zhuanlan.zhihu.com/p/610171544\n",
    "使用BERT进行特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad044e0-06e1-49fd-9524-8e03a0a81e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.19.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这里导入transformers的包，然后运行这段代码，如果安装成功的话会显示版本号\n",
    "import transformers\n",
    "transformers.__version__   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd35391-455b-4081-8fa7-cf0e150c1492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),\n",
       " ['选择珠江花园的原因就是方便。',\n",
       "  '笔记本的键盘确实爽。',\n",
       "  '房间太小。其他的都一般。',\n",
       "  '今天才知道这书还有第6卷,真有点郁闷.',\n",
       "  '机器背面似乎被撕了张什么标签，残胶还在。'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载预训练字典和分词方法\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',  # 可选，huggingface 中的预训练模型名称或路径，默认为 bert-base-chinese\n",
    "    cache_dir=None,  # 将数据保存到的本地位置，使用cache_dir 可以指定文件下载位置\n",
    "    force_download=False,   \n",
    ")\n",
    "\n",
    "sents = [\n",
    "    '选择珠江花园的原因就是方便。',\n",
    "    '笔记本的键盘确实爽。',\n",
    "    '房间太小。其他的都一般。',\n",
    "    '今天才知道这书还有第6卷,真有点郁闷.',\n",
    "    '机器背面似乎被撕了张什么标签，残胶还在。',\n",
    "]\n",
    "\n",
    "tokenizer, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2d3303-5bb1-4be7-a250-23edb84d72e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#编码两个句子\n",
    "out = tokenizer.encode(\n",
    "    text=sents[0],\n",
    "    text_pair=sents[1],  # 一次编码两个句子，若没有text_pair这个参数，就一次编码一个句子\n",
    "\n",
    "    #当句子长度大于max_length时,截断\n",
    "    truncation=True,\n",
    "\n",
    "    #一律补pad到max_length长度\n",
    "    padding='max_length',   # 少于max_length时就padding\n",
    "    add_special_tokens=True,\n",
    "    max_length=30,\n",
    "    return_tensors=None,  # None表示不指定数据类型，默认返回list\n",
    ")\n",
    "\n",
    "print(out)\n",
    "\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7634d-8ad5-49aa-af02-37952c805b3f",
   "metadata": {},
   "source": [
    "主要的参数解释如下:  \n",
    "text和text_pair:传入要编码的一句话或两句话文本。  \n",
    "truncation:当句子长度大于max_length时,是否进行截断。True表示进行截断。  \n",
    "padding:'max_length'表示句子长度小于max_length时,使用pad token在句子末尾补全到max_length长度。  \n",
    "add_special_tokens:是否添加特殊字符,如[CLS]等。  \n",
    "max_length:最大长度,若超过该长度则会截断。  \n",
    "return_tensors:返回的数据类型,None表示返回数字list。  \n",
    "\n",
    "编码的主要步骤是:  \n",
    "使用词典,将文本tokenize成单词列表,比如['我','爱','北','京']  \n",
    "添加特殊字符,如'[CLS]' ,'[SEP]'等。  \n",
    "截断或padding句子到max_length长度。  \n",
    "将单词映射为词典中的id。  \n",
    "返回list或指定类型的张量。  \n",
    "decode逆向操作,将编码后的id序列转换回文本。  \n",
    "\n",
    "这种编码方式可以处理单句或双句的输入,并将其转换为模型可接受的标准长度序列输入。后续这种编码后的输入可直接用于模型训练或者预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f3e523-ce9e-4a2f-b8b5-3bf1922b3731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#增强的编码函数\n",
    "out = tokenizer.encode_plus(\n",
    "    text=sents[0],\n",
    "    text_pair=sents[1],\n",
    "\n",
    "    #当句子长度大于max_length时,截断\n",
    "    truncation=True,\n",
    "\n",
    "    #一律补零到max_length长度\n",
    "    padding='max_length',\n",
    "    max_length=30,\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    #可取值tensorflow,pytorch,numpy,默认值None为返回list\n",
    "    return_tensors=None,\n",
    "\n",
    "    #返回token_type_ids\n",
    "    return_token_type_ids=True,\n",
    "\n",
    "    #返回attention_mask\n",
    "    return_attention_mask=True,\n",
    "\n",
    "    #返回special_tokens_mask 特殊符号标识\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
    "    #return_offsets_mapping=True,\n",
    "\n",
    "    #返回length 标识长度\n",
    "    return_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7e1ea-75eb-4ac2-9d76-71721955eed3",
   "metadata": {},
   "source": [
    "这个是对BertTokenizer的encode函数的增强,它不仅返回编码后的序列,还会返回一些额外的信息,比如token类型、attention mask等,这些信息在BERT模型的训练和预测中会用到。\n",
    "\n",
    "主要的返回值说明:  \n",
    "input_ids: 经过WordPiece编码的输入序列,就是encode的直接输出。  \n",
    "token_type_ids: 区分第一个第二个句子的标志,用于表示是 encoder的输入是单句还是双句。  \n",
    "attention_mask: 标记padding部分,让self-attention只关注非padding部分。  \n",
    "special_tokens_mask: 标记special tokens的mask,比如[CLS]等。  \n",
    "return_length: 返回输入的长度。  \n",
    "所以encode_plus相比encode,提供了更全面的信息,包括输入序列、输入长度、标记不同部分的mask等,这些信息在BERT模型的训练和预测中都是必要的。  \n",
    "\n",
    "调用返回的是字典类型,包含以上各个编码信息。我们可以直接拿这个字典喂给BERT模型,而不需要自己去计算mask、长度等信息。这样可以简化使用BERT的流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7691f9-c4fa-4111-9f6d-325e34ffc38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]\n",
      "token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "special_tokens_mask : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "length : 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619dc325-2f38-4e77-8b3d-2912caefc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#批量编码一个一个的句子\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[sents[0], sents[1]],  # 批量编码，一次编码了两个句子(与增强的编码函数相比，就此处不同)\n",
    "\n",
    "    #当句子长度大于max_length时,截断\n",
    "    truncation=True,\n",
    "\n",
    "    #一律补零到max_length长度\n",
    "    padding='max_length',\n",
    "    max_length=15,\n",
    "    add_special_tokens=True,\n",
    "    \n",
    "    #可取值tf,pt,np,默认为返回list\n",
    "    return_tensors=None,\n",
    "\n",
    "    #返回token_type_ids\n",
    "    return_token_type_ids=True,\n",
    "\n",
    "    #返回attention_mask\n",
    "    return_attention_mask=True,\n",
    "\n",
    "    #返回special_tokens_mask 特殊符号标识\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
    "    #return_offsets_mapping=True,\n",
    "\n",
    "    #返回length 标识长度\n",
    "    return_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d095a5-8ec4-4d46-a42d-372512c93401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 102], [101, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]\n",
      "length : [15, 12]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 [SEP]',\n",
       " '[CLS] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out['input_ids'][0]), tokenizer.decode(out['input_ids'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d94d2cc-2341-48dc-aa2b-d61bb3fa5ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "#批量编码成对的句子\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[(sents[0], sents[1]), (sents[2], sents[3])],   # tuple\n",
    "\n",
    "    #当句子长度大于max_length时,截断\n",
    "    truncation=True,\n",
    "\n",
    "    #一律补零到max_length长度\n",
    "    padding='max_length',\n",
    "    max_length=30,\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    #可取值tf,pt,np,默认为返回list\n",
    "    return_tensors=None,\n",
    "\n",
    "    #返回token_type_ids\n",
    "    return_token_type_ids=True,\n",
    "\n",
    "    #返回attention_mask\n",
    "    return_attention_mask=True,\n",
    "\n",
    "    #返回special_tokens_mask 特殊符号标识\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
    "    #return_offsets_mapping=True,\n",
    "\n",
    "    #返回length 标识长度\n",
    "    return_length=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2629fbb4-4a6a-4c29-a4d2-1d366e408167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0], [101, 2791, 7313, 1922, 2207, 511, 1071, 800, 4638, 6963, 671, 5663, 511, 102, 791, 1921, 2798, 4761, 6887, 6821, 741, 6820, 3300, 5018, 127, 1318, 117, 4696, 3300, 102]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "length : [27, 30]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfaa0d77-93d0-4f93-98b6-3d906dd71de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 21128, False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取字典\n",
    "zidian = tokenizer.get_vocab()\n",
    "\n",
    "type(zidian), len(zidian), '月光' in zidian,   # (dict, 21128, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f28555-f90e-42b2-a2a0-5ff861ea2fd9",
   "metadata": {},
   "source": [
    "这里我们调用了get_vocab()方法来获取分词器的词典。词典是一个字典类型,键是词,值是词对应的数字索引。\n",
    "\n",
    "我们可以看到:  \n",
    "\n",
    "词典是dict类型   \n",
    "词典大小是21128个词  \n",
    "词典中不包含'月光'这个词  \n",
    "这验证了BERT使用的是WordPiece算法进行分词,词典中包含字母、词汇和词组,但不会包含所有汉语词汇。  \n",
    "\n",
    "通过获取词典我们可以了解到分词器的词汇量和词汇覆盖情况,也可以直接查询某个词是否在词典中。这对分析BERT语言表示能力以及优化词典非常有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84edd9bd-4c3c-4384-aed7-3f10fc56a63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 21131, 21128, 21130, 21129)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#添加新词\n",
    "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
    "\n",
    "#添加新符号\n",
    "tokenizer.add_special_tokens({'eos_token': '[EOS]'})   # End Of Sentence\n",
    "\n",
    "zidian = tokenizer.get_vocab()\n",
    "\n",
    "type(zidian), len(zidian), zidian['月光'], zidian['[EOS]'],zidian['希望']   # (dict, 21131, 21128, 21130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb533d91-0886-4bd9-9507-fd3a96ed95c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 21128, 4638, 3173, 21129, 21130, 102, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 月光 的 新 希望 [EOS] [SEP] [PAD]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#编码新添加的词\n",
    "out = tokenizer.encode(\n",
    "    text='月光的新希望[EOS]',\n",
    "    text_pair=None,\n",
    "\n",
    "    #当句子长度大于max_length时,截断\n",
    "    truncation=True,\n",
    "\n",
    "    #一律补pad到max_length长度\n",
    "    padding='max_length',\n",
    "    add_special_tokens=True,\n",
    "    max_length=8,\n",
    "    \n",
    "    return_tensors=None,\n",
    ")\n",
    "\n",
    "print(out)\n",
    "\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a51025f-15d4-4368-bd00-87c83eb66d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/apple/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-eaea6a9750cb0fe7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b915d57687471bbddffd8d21241a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ChnSentiCorp数据集包含正面和负面评论句子,可以用来进行中文文本的情感分析。\n",
    "#https://github.com/brightmart/nlp_chinese_corpus\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "#加载数据\n",
    "dataset = load_dataset(path='lansinuote/ChnSentiCorp')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3be66a58-7835-4703-858a-ffa70e227252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#保存数据集到磁盘\n",
    "dataset.save_to_disk(dataset_dict_path='./data/ChnSentiCorp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da6d26af-e7d9-45be-8844-1de1a3761cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#取出训练集\n",
    "dataset = dataset['train']\n",
    "#查看一个数据\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "137d743f-675c-4429-a664-a0e72b0f446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j9/ryqgqp6n03schy65y0b0g4sm0000gn/T/ipykernel_42992/1035245866.py:4: FutureWarning: list_metrics is deprecated and will be removed in the next major version of datasets. Use 'evaluate.list_evaluation_modules' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metrics_list = list_metrics()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(119,\n",
       " ['accuracy',\n",
       "  'bertscore',\n",
       "  'bleu',\n",
       "  'bleurt',\n",
       "  'brier_score',\n",
       "  'cer',\n",
       "  'character',\n",
       "  'charcut_mt',\n",
       "  'chrf',\n",
       "  'code_eval',\n",
       "  'comet',\n",
       "  'competition_math',\n",
       "  'coval',\n",
       "  'cuad',\n",
       "  'exact_match',\n",
       "  'f1',\n",
       "  'frugalscore',\n",
       "  'glue',\n",
       "  'google_bleu',\n",
       "  'indic_glue',\n",
       "  'mae',\n",
       "  'mahalanobis',\n",
       "  'mape',\n",
       "  'mase',\n",
       "  'matthews_correlation',\n",
       "  'mauve',\n",
       "  'mean_iou',\n",
       "  'meteor',\n",
       "  'mse',\n",
       "  'nist_mt',\n",
       "  'pearsonr',\n",
       "  'perplexity',\n",
       "  'poseval',\n",
       "  'precision',\n",
       "  'r_squared',\n",
       "  'recall',\n",
       "  'rl_reliability',\n",
       "  'roc_auc',\n",
       "  'rouge',\n",
       "  'sacrebleu',\n",
       "  'sari',\n",
       "  'seqeval',\n",
       "  'smape',\n",
       "  'spearmanr',\n",
       "  'squad',\n",
       "  'squad_v2',\n",
       "  'super_glue',\n",
       "  'ter',\n",
       "  'trec_eval',\n",
       "  'wer',\n",
       "  'wiki_split',\n",
       "  'xnli',\n",
       "  'xtreme_s',\n",
       "  'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics',\n",
       "  'BucketHeadP65/confusion_matrix',\n",
       "  'BucketHeadP65/roc_curve',\n",
       "  'Drunper/metrica_tesi',\n",
       "  'Felipehonorato/eer',\n",
       "  'He-Xingwei/sari_metric',\n",
       "  'JP-SystemsX/nDCG',\n",
       "  'Josh98/nl2bash_m',\n",
       "  'Kyle1668/squad',\n",
       "  'Muennighoff/code_eval',\n",
       "  'NCSOFT/harim_plus',\n",
       "  'Natooz/ece',\n",
       "  'NikitaMartynov/spell-check-metric',\n",
       "  'Pipatpong/perplexity',\n",
       "  'Splend1dchan/cosine_similarity',\n",
       "  'Viona/fuzzy_reordering',\n",
       "  'Viona/kendall_tau',\n",
       "  'Vipitis/shadermatch',\n",
       "  'Yeshwant123/mcc',\n",
       "  'abdusah/aradiawer',\n",
       "  'abidlabs/mean_iou',\n",
       "  'abidlabs/mean_iou2',\n",
       "  'andstor/code_perplexity',\n",
       "  'angelina-wang/directional_bias_amplification',\n",
       "  'aryopg/roc_auc_skip_uniform_labels',\n",
       "  'brian920128/doc_retrieve_metrics',\n",
       "  'bstrai/classification_report',\n",
       "  'chanelcolgate/average_precision',\n",
       "  'ckb/unigram',\n",
       "  'codeparrot/apps_metric',\n",
       "  'cpllab/syntaxgym',\n",
       "  'dvitel/codebleu',\n",
       "  'ecody726/bertscore',\n",
       "  'fschlatt/ner_eval',\n",
       "  'giulio98/codebleu',\n",
       "  'guydav/restrictedpython_code_eval',\n",
       "  'harshhpareek/bertscore',\n",
       "  'hpi-dhc/FairEval',\n",
       "  'hynky/sklearn_proxy',\n",
       "  'hyperml/balanced_accuracy',\n",
       "  'ingyu/klue_mrc',\n",
       "  'jpxkqx/peak_signal_to_noise_ratio',\n",
       "  'jpxkqx/signal_to_reconstruction_error',\n",
       "  'k4black/codebleu',\n",
       "  'kaggle/ai4code',\n",
       "  'langdonholmes/cohen_weighted_kappa',\n",
       "  'lhy/hamming_loss',\n",
       "  'lhy/ranking_loss',\n",
       "  'lvwerra/accuracy_score',\n",
       "  'manueldeprada/beer',\n",
       "  'mfumanelli/geometric_mean',\n",
       "  'omidf/squad_precision_recall',\n",
       "  'posicube/mean_reciprocal_rank',\n",
       "  'sakusakumura/bertscore',\n",
       "  'sma2023/wil',\n",
       "  'spidyidcccc/bertscore',\n",
       "  'tialaeMceryu/unigram',\n",
       "  'transZ/sbert_cosine',\n",
       "  'transZ/test_parascore',\n",
       "  'transformersegmentation/segmentation_scores',\n",
       "  'unitxt/metric',\n",
       "  'unnati/kendall_tau_distance',\n",
       "  'weiqis/pajm',\n",
       "  'ybelkada/cocoevaluate',\n",
       "  'yonting/average_precision_score',\n",
       "  'yuyijiong/quad_match_score'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import list_metrics\n",
    "\n",
    "#列出评价指标\n",
    "metrics_list = list_metrics()\n",
    "len(metrics_list), metrics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea2727-dc47-4c5b-8056-df95343a9d16",
   "metadata": {},
   "source": [
    "这里使用datasets库的list_metrics函数列出了所有内置的评价指标。这样就不用自己找函数定义评价指标或者计算评价指标了。  \n",
    "使用list_metrics可以快速查看datasets当前支持的所有评价指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebaae2d1-0d96-4535-95ca-c8e9c67bb0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j9/ryqgqp6n03schy65y0b0g4sm0000gn/T/ipykernel_42992/1543994463.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', 'mrpc')   # MRPC(The Microsoft Research Paraphrase Corpus，微软研究院释义语料库)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
      "Args:\n",
      "    predictions: list of predictions to score.\n",
      "        Each translation should be tokenized into a list of tokens.\n",
      "    references: list of lists of references for each translation.\n",
      "        Each reference should be tokenized into a list of tokens.\n",
      "Returns: depending on the GLUE subset, one or several of:\n",
      "    \"accuracy\": Accuracy\n",
      "    \"f1\": F1 score\n",
      "    \"pearson\": Pearson Correlation\n",
      "    \"spearmanr\": Spearman Correlation\n",
      "    \"matthews_correlation\": Matthew Correlation\n",
      "Examples:\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0, 'f1': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
      "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
      "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'matthews_correlation': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "#加载一个评价指标\n",
    "metric = load_metric('glue', 'mrpc')   # MRPC(The Microsoft Research Paraphrase Corpus，微软研究院释义语料库)\n",
    "print(metric.inputs_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fdfcdc1-a07d-4292-adbd-c3e8761a4738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666, 'f1': 0.6666666666666666}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算一个评价指标\n",
    "predictions = [0, 1, 0]\n",
    "references = [0, 1, 1]\n",
    "\n",
    "final_score = metric.compute(predictions=predictions, references=references)\n",
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "120510f9-19fc-4411-9b13-a2ab13a5ffa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/apple/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-eaea6a9750cb0fe7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9600,\n",
       " ('选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "  1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        # 加载数据。ChnSentiCorp为消费评价数据集，分好评和差评\n",
    "        self.dataset = load_dataset(path='lansinuote/ChnSentiCorp', split=split)  # Available splits: ['test', 'train', 'validation']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "        return text, label\n",
    "\n",
    "dataset = Dataset('train')\n",
    "len(dataset), dataset[0]  # 训练集有9600句话；dataset[0]表示第1句话，前面是text，后面是label，1表示差评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7a1e7fa-5154-467a-a6fa-7e04a32a0408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具，即tokenizer\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')  # 要跟预训练模型相匹配\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b0c16f4-e92b-4bc5-98b8-3654bb5b2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义批处理函数\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]#提取文本\n",
    "    labels = [i[1] for i in data]#提取标签\n",
    "\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,   # 当句子长度大于max_length时，截断\n",
    "                                   padding='max_length',   # 一律补0到max_length长度\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt',   # 返回pytorch类型的tensor\n",
    "                                   return_length=True)   # 返回length，标识长度\n",
    "\n",
    "    input_ids = data['input_ids']    # input_ids:编码之后的数字\n",
    "    attention_mask = data['attention_mask']     # attention_mask:补零的位置是0,其他位置是1\n",
    "    token_type_ids = data['token_type_ids']   # 第一个句子和特殊符号的位置是0，第二个句子的位置是1(包括第二个句子后的[SEP])\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c53e583-53ce-4597-87c6-a5be909b7c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "# 查看数据样例\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))  # 600 = 9600/16\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels   # 500表示句子最大长度为500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2320d422-222d-4501-af80-bc72af5f9880",
   "metadata": {},
   "source": [
    "定义了一个DataLoader,参数包括:  \n",
    "dataset:要加载的数据集  \n",
    "batch_size:批大小为16  \n",
    "collate_fn:使用之前定义好的collate函数进行批处理  \n",
    "shuffle:打乱数据  \n",
    "drop_last:去掉最后不完整的batch  \n",
    "遍历loader,查看一个batch的数据样例  \n",
    "\n",
    "打印loader的长度,以及一个batch中各个tensor的shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd5e18ea-ad51-4d29-a4ef-e582f3eefb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "print(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "386c185e-4ad5-4cb5-9e1f-d93d37a18cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)  # 这里不使用fine-tuning，直接把预训练模型的参数冻结住，只训练下游任务模型，对预训练模型本身的参数不调整"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694369a-35af-47eb-8a23-42c99a35ba20",
   "metadata": {},
   "source": [
    "这段代码的目的是冻结预训练语言模型(如BERT)的参数,使其在训练下游任务时不更新。\n",
    "\n",
    "主要步骤是:\n",
    "\n",
    "遍历预训练模型pretrained中的所有参数param。\n",
    "\n",
    "对每个参数调用requires_grad_(False),将requires_grad标志置为False。\n",
    "\n",
    "这样在训练时计算图就不会跟踪这些参数的梯度,实现了冻结预训练参数的效果。\n",
    "\n",
    "这个技巧通常用在使用预训练语言模型做下游任务的时候,先冻结预训练模型,只训练新增的参数,然后再解冻后整体微调。\n",
    "\n",
    "它可以防止预训练参数在下游任务训练初期被破坏,并且加速收敛。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05d54320-d7cb-4fc4-8aa6-cda110ed2917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 500, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型试算\n",
    "out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "\n",
    "out.last_hidden_state.shape   # [batch_size, 数据分词的长度(每一句话编码成500个词的长度), 词编码的维度(即每一个词编码成一个768维的向量)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9e1be-61b3-4df3-801d-51308394559e",
   "metadata": {},
   "source": [
    "这里使用了预训练语言模型(如BERT)对输入进行了前向计算,并打印出输出tensor的shape。\n",
    "\n",
    "主要的步骤是:  \n",
    "将经过分词和编码后的input作为输入,包括input_ids,attention_mask,token_type_ids。  \n",
    "把这些输入传给预训练模型pretrained,调用模型执行前向计算。  \n",
    "从输出out中取出最后一层的隐状态last_hidden_state。  \n",
    "打印出last_hidden_state的shape。  \n",
    "\n",
    "从打印的shape可以看出:  \n",
    "第一个维度是batch size,即一次输入的样本数量。  \n",
    "第二个维度是输入序列长度,经过分词和truncation/padding后都处理成了固定长度。  \n",
    "第三个维度是词向量维度,即BERT模型对每个词编码成的向量表示维度。  \n",
    "所以输出tensor的形状反映了模型处理批量序列输入的结果。\n",
    "\n",
    "每次向BERT等预训练语言模型提供输入时,都需要按照这个格式提供。  \n",
    "具体来说,输入对应的模型部分是:  \n",
    "input_ids:对应embedding层的输入,表示单词的索引信息。  \n",
    "attention_mask:对应self-attention层的mask输入,指示哪些词是padding,哪些词是有效词。  \n",
    "token_type_ids:对应第一句和第二句的区分,让模型学习句子关系。  \n",
    "这三个输入分别提供了输入序列、标记和句子关系信息。它们共同定义了模型的文本输入。\n",
    "\n",
    "模型内部的计算流程是:  \n",
    "输入层根据input_ids查表获取词向量表示;  \n",
    "多层Transformer编码器结构捕获上下文信息;  \n",
    "最终输出包含句子级语义信息的词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "121b6f95-d517-4c42-98b1-d085df0ba534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)  # 单层网络模型，只包括了一个fc的神经网络\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,   # 先拿预训练模型来做一个计算，抽取数据当中的特征\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "\n",
    "        # 把抽取出来的特征放到全连接网络中运算，且特征的结果只需要第0个词的特征(跟bert模型的设计方式有关。对句子的情感分类，只需要拿特征中的第0个词来进行分类就可以了)\n",
    "        out = self.fc(out.last_hidden_state[:, 0])   # torch.Size([16, 768])\n",
    "        \n",
    "        # 将softmax函数应用于一个n维输入张量，对其进行缩放，使n维输出张量的元素位于[0,1]范围内，总和为1\n",
    "        out = out.softmax(dim=1)  \n",
    "\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape    # torch.Size([16, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff41425-dd50-4d06-a56f-8397232ccc12",
   "metadata": {},
   "source": [
    "这个定义的这个下游任务模型实现了基于BERT的文本分类,主要步骤如下:\n",
    "定义Model类,继承torch.nn.Module。  \n",
    "初始化时定义了一个线性层fc,输入是BERT输出维度768,输出是2分类。  \n",
    "forward函数中,首先使用torch.no_grad()不计算梯度地运行BERT获取特征。  \n",
    "然后取最后一层隐状态的第一个token作为分类特征,输入到fc层。  \n",
    "对fc层输出使用softmax获得归一化的预测分布。  \n",
    "这样即实现了利用BERT提取特征,然后接一个简单的线性分类器来进行文本分类。\n",
    "\n",
    "取最后一层第一个token作为句子表示是BERT特有的做法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17c858b2-645a-43a0-96a1-581bb8e41191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7090100049972534 0.375\n",
      "5 0.6451802253723145 0.75\n",
      "10 0.6374646425247192 0.6875\n",
      "15 0.6967511773109436 0.5625\n",
      "20 0.6140915155410767 0.75\n",
      "25 0.5840007066726685 0.75\n",
      "30 0.5491213202476501 0.9375\n",
      "35 0.6162095665931702 0.75\n",
      "40 0.5880313515663147 0.625\n",
      "45 0.5858539938926697 0.75\n",
      "50 0.5374869704246521 0.875\n",
      "55 0.5708539485931396 0.75\n",
      "60 0.533920407295227 0.8125\n",
      "65 0.5271445512771606 0.875\n",
      "70 0.49780941009521484 0.875\n",
      "75 0.5622323155403137 0.875\n",
      "80 0.5095774531364441 0.875\n",
      "85 0.47063907980918884 0.875\n",
      "90 0.4942939281463623 0.8125\n",
      "95 0.4704240560531616 0.9375\n",
      "100 0.4980355203151703 0.8125\n",
      "105 0.4693685472011566 0.9375\n",
      "110 0.4152890145778656 1.0\n",
      "115 0.4286099970340729 0.9375\n",
      "120 0.4034077227115631 1.0\n",
      "125 0.44301509857177734 1.0\n",
      "130 0.5175010561943054 0.75\n",
      "135 0.4954994022846222 0.875\n",
      "140 0.45595037937164307 0.9375\n",
      "145 0.4319436550140381 0.9375\n",
      "150 0.5459808111190796 0.875\n",
      "155 0.47220784425735474 0.875\n",
      "160 0.3745757043361664 0.9375\n",
      "165 0.48836103081703186 0.8125\n",
      "170 0.5114022493362427 0.875\n",
      "175 0.3824004530906677 1.0\n",
      "180 0.4282243847846985 0.875\n",
      "185 0.5618668794631958 0.6875\n",
      "190 0.42371630668640137 0.9375\n",
      "195 0.4668157696723938 0.875\n",
      "200 0.44685202836990356 0.9375\n",
      "205 0.4461783468723297 0.875\n",
      "210 0.44988885521888733 0.875\n",
      "215 0.42966315150260925 0.9375\n",
      "220 0.4938286542892456 0.8125\n",
      "225 0.4855901598930359 0.875\n",
      "230 0.46268633008003235 0.875\n",
      "235 0.47675976157188416 0.875\n",
      "240 0.4031706750392914 0.9375\n",
      "245 0.5391350388526917 0.75\n",
      "250 0.48207417130470276 0.875\n",
      "255 0.4860449433326721 0.8125\n",
      "260 0.3749210238456726 1.0\n",
      "265 0.5656070113182068 0.8125\n",
      "270 0.4984758794307709 0.875\n",
      "275 0.4937138855457306 0.875\n",
      "280 0.5697503685951233 0.75\n",
      "285 0.40729373693466187 0.9375\n",
      "290 0.49484023451805115 0.8125\n",
      "295 0.5590804219245911 0.8125\n",
      "300 0.37672945857048035 0.9375\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW   # 优化器，即 Adam + Weight decay(自适应梯度方法)\n",
    "\n",
    "#训练下游任务模型\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Pytorch计算交叉熵误差的函数自带softmax，故训练时模型里不要添加softmax\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "    loss = criterion(out, labels)  # 输出跟真实的labels计算loss\n",
    "    loss.backward()   # 调用反向传播得到每个要更新参数的梯度\n",
    "    optimizer.step()  # 每个参数根据上一步得到的梯度进行优化\n",
    "    optimizer.zero_grad()  # 把上一步训练的每个参数的梯度清零\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "        print(i, loss.item(), accuracy)\n",
    "        if i == 300:  # 只训练300个轮次，没有把全量的数据(len(loader)= 600 = 9600/16)全部训练一遍\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c765ed69-7d70-4e8a-b42e-fcf3bdd9320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/apple/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-eaea6a9750cb0fe7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.85625\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        if i == 5:\n",
    "            break   # 只测试前5个\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)  # 测试集的正确率\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ace914-f2d1-474f-84f7-ec4bdd54ad44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
