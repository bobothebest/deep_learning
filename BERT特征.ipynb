{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95381fa6-ff56-40f5-9dd0-bd678c9a62f0",
   "metadata": {},
   "source": [
    "ä¸»è¦å‚è€ƒï¼šhttps://zhuanlan.zhihu.com/p/610171544\n",
    "ä½¿ç”¨BERTè¿›è¡Œç‰¹å¾æå–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad044e0-06e1-49fd-9524-8e03a0a81e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.19.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#è¿™é‡Œå¯¼å…¥transformersçš„åŒ…ï¼Œç„¶åè¿è¡Œè¿™æ®µä»£ç ï¼Œå¦‚æœå®‰è£…æˆåŠŸçš„è¯ä¼šæ˜¾ç¤ºç‰ˆæœ¬å·\n",
    "import transformers\n",
    "transformers.__version__   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd35391-455b-4081-8fa7-cf0e150c1492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),\n",
       " ['é€‰æ‹©ç æ±ŸèŠ±å›­çš„åŸå› å°±æ˜¯æ–¹ä¾¿ã€‚',\n",
       "  'ç¬”è®°æœ¬çš„é”®ç›˜ç¡®å®çˆ½ã€‚',\n",
       "  'æˆ¿é—´å¤ªå°ã€‚å…¶ä»–çš„éƒ½ä¸€èˆ¬ã€‚',\n",
       "  'ä»Šå¤©æ‰çŸ¥é“è¿™ä¹¦è¿˜æœ‰ç¬¬6å·,çœŸæœ‰ç‚¹éƒé—·.',\n",
       "  'æœºå™¨èƒŒé¢ä¼¼ä¹è¢«æ’•äº†å¼ ä»€ä¹ˆæ ‡ç­¾ï¼Œæ®‹èƒ¶è¿˜åœ¨ã€‚'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#åŠ è½½é¢„è®­ç»ƒå­—å…¸å’Œåˆ†è¯æ–¹æ³•\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',  # å¯é€‰ï¼Œhuggingface ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è·¯å¾„ï¼Œé»˜è®¤ä¸º bert-base-chinese\n",
    "    cache_dir=None,  # å°†æ•°æ®ä¿å­˜åˆ°çš„æœ¬åœ°ä½ç½®ï¼Œä½¿ç”¨cache_dir å¯ä»¥æŒ‡å®šæ–‡ä»¶ä¸‹è½½ä½ç½®\n",
    "    force_download=False,   \n",
    ")\n",
    "\n",
    "sents = [\n",
    "    'é€‰æ‹©ç æ±ŸèŠ±å›­çš„åŸå› å°±æ˜¯æ–¹ä¾¿ã€‚',\n",
    "    'ç¬”è®°æœ¬çš„é”®ç›˜ç¡®å®çˆ½ã€‚',\n",
    "    'æˆ¿é—´å¤ªå°ã€‚å…¶ä»–çš„éƒ½ä¸€èˆ¬ã€‚',\n",
    "    'ä»Šå¤©æ‰çŸ¥é“è¿™ä¹¦è¿˜æœ‰ç¬¬6å·,çœŸæœ‰ç‚¹éƒé—·.',\n",
    "    'æœºå™¨èƒŒé¢ä¼¼ä¹è¢«æ’•äº†å¼ ä»€ä¹ˆæ ‡ç­¾ï¼Œæ®‹èƒ¶è¿˜åœ¨ã€‚',\n",
    "]\n",
    "\n",
    "tokenizer, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2d3303-5bb1-4be7-a250-23edb84d72e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] é€‰ æ‹© ç  æ±Ÿ èŠ± å›­ çš„ åŸ å›  å°± æ˜¯ æ–¹ ä¾¿ ã€‚ [SEP] ç¬” è®° æœ¬ çš„ é”® ç›˜ ç¡® å® çˆ½ ã€‚ [SEP] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ç¼–ç ä¸¤ä¸ªå¥å­\n",
    "out = tokenizer.encode(\n",
    "    text=sents[0],\n",
    "    text_pair=sents[1],  # ä¸€æ¬¡ç¼–ç ä¸¤ä¸ªå¥å­ï¼Œè‹¥æ²¡æœ‰text_pairè¿™ä¸ªå‚æ•°ï¼Œå°±ä¸€æ¬¡ç¼–ç ä¸€ä¸ªå¥å­\n",
    "\n",
    "    #å½“å¥å­é•¿åº¦å¤§äºmax_lengthæ—¶,æˆªæ–­\n",
    "    truncation=True,\n",
    "\n",
    "    #ä¸€å¾‹è¡¥padåˆ°max_lengthé•¿åº¦\n",
    "    padding='max_length',   # å°‘äºmax_lengthæ—¶å°±padding\n",
    "    add_special_tokens=True,\n",
    "    max_length=30,\n",
    "    return_tensors=None,  # Noneè¡¨ç¤ºä¸æŒ‡å®šæ•°æ®ç±»å‹ï¼Œé»˜è®¤è¿”å›list\n",
    ")\n",
    "\n",
    "print(out)\n",
    "\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7634d-8ad5-49aa-af02-37952c805b3f",
   "metadata": {},
   "source": [
    "ä¸»è¦çš„å‚æ•°è§£é‡Šå¦‚ä¸‹:  \n",
    "textå’Œtext_pair:ä¼ å…¥è¦ç¼–ç çš„ä¸€å¥è¯æˆ–ä¸¤å¥è¯æ–‡æœ¬ã€‚  \n",
    "truncation:å½“å¥å­é•¿åº¦å¤§äºmax_lengthæ—¶,æ˜¯å¦è¿›è¡Œæˆªæ–­ã€‚Trueè¡¨ç¤ºè¿›è¡Œæˆªæ–­ã€‚  \n",
    "padding:'max_length'è¡¨ç¤ºå¥å­é•¿åº¦å°äºmax_lengthæ—¶,ä½¿ç”¨pad tokenåœ¨å¥å­æœ«å°¾è¡¥å…¨åˆ°max_lengthé•¿åº¦ã€‚  \n",
    "add_special_tokens:æ˜¯å¦æ·»åŠ ç‰¹æ®Šå­—ç¬¦,å¦‚[CLS]ç­‰ã€‚  \n",
    "max_length:æœ€å¤§é•¿åº¦,è‹¥è¶…è¿‡è¯¥é•¿åº¦åˆ™ä¼šæˆªæ–­ã€‚  \n",
    "return_tensors:è¿”å›çš„æ•°æ®ç±»å‹,Noneè¡¨ç¤ºè¿”å›æ•°å­—listã€‚  \n",
    "\n",
    "ç¼–ç çš„ä¸»è¦æ­¥éª¤æ˜¯:  \n",
    "ä½¿ç”¨è¯å…¸,å°†æ–‡æœ¬tokenizeæˆå•è¯åˆ—è¡¨,æ¯”å¦‚['æˆ‘','çˆ±','åŒ—','äº¬']  \n",
    "æ·»åŠ ç‰¹æ®Šå­—ç¬¦,å¦‚'[CLS]' ,'[SEP]'ç­‰ã€‚  \n",
    "æˆªæ–­æˆ–paddingå¥å­åˆ°max_lengthé•¿åº¦ã€‚  \n",
    "å°†å•è¯æ˜ å°„ä¸ºè¯å…¸ä¸­çš„idã€‚  \n",
    "è¿”å›listæˆ–æŒ‡å®šç±»å‹çš„å¼ é‡ã€‚  \n",
    "decodeé€†å‘æ“ä½œ,å°†ç¼–ç åçš„idåºåˆ—è½¬æ¢å›æ–‡æœ¬ã€‚  \n",
    "\n",
    "è¿™ç§ç¼–ç æ–¹å¼å¯ä»¥å¤„ç†å•å¥æˆ–åŒå¥çš„è¾“å…¥,å¹¶å°†å…¶è½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ‡å‡†é•¿åº¦åºåˆ—è¾“å…¥ã€‚åç»­è¿™ç§ç¼–ç åçš„è¾“å…¥å¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒæˆ–è€…é¢„æµ‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f3e523-ce9e-4a2f-b8b5-3bf1922b3731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¢å¼ºçš„ç¼–ç å‡½æ•°\n",
    "out = tokenizer.encode_plus(\n",
    "    text=sents[0],\n",
    "    text_pair=sents[1],\n",
    "\n",
    "    #å½“å¥å­é•¿åº¦å¤§äºmax_lengthæ—¶,æˆªæ–­\n",
    "    truncation=True,\n",
    "\n",
    "    #ä¸€å¾‹è¡¥é›¶åˆ°max_lengthé•¿åº¦\n",
    "    padding='max_length',\n",
    "    max_length=30,\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    #å¯å–å€¼tensorflow,pytorch,numpy,é»˜è®¤å€¼Noneä¸ºè¿”å›list\n",
    "    return_tensors=None,\n",
    "\n",
    "    #è¿”å›token_type_ids\n",
    "    return_token_type_ids=True,\n",
    "\n",
    "    #è¿”å›attention_mask\n",
    "    return_attention_mask=True,\n",
    "\n",
    "    #è¿”å›special_tokens_mask ç‰¹æ®Šç¬¦å·æ ‡è¯†\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #è¿”å›offset_mapping æ ‡è¯†æ¯ä¸ªè¯çš„èµ·æ­¢ä½ç½®,è¿™ä¸ªå‚æ•°åªèƒ½BertTokenizerFastä½¿ç”¨\n",
    "    #return_offsets_mapping=True,\n",
    "\n",
    "    #è¿”å›length æ ‡è¯†é•¿åº¦\n",
    "    return_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7e1ea-75eb-4ac2-9d76-71721955eed3",
   "metadata": {},
   "source": [
    "è¿™ä¸ªæ˜¯å¯¹BertTokenizerçš„encodeå‡½æ•°çš„å¢å¼º,å®ƒä¸ä»…è¿”å›ç¼–ç åçš„åºåˆ—,è¿˜ä¼šè¿”å›ä¸€äº›é¢å¤–çš„ä¿¡æ¯,æ¯”å¦‚tokenç±»å‹ã€attention maskç­‰,è¿™äº›ä¿¡æ¯åœ¨BERTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ä¸­ä¼šç”¨åˆ°ã€‚\n",
    "\n",
    "ä¸»è¦çš„è¿”å›å€¼è¯´æ˜:  \n",
    "input_ids: ç»è¿‡WordPieceç¼–ç çš„è¾“å…¥åºåˆ—,å°±æ˜¯encodeçš„ç›´æ¥è¾“å‡ºã€‚  \n",
    "token_type_ids: åŒºåˆ†ç¬¬ä¸€ä¸ªç¬¬äºŒä¸ªå¥å­çš„æ ‡å¿—,ç”¨äºè¡¨ç¤ºæ˜¯ encoderçš„è¾“å…¥æ˜¯å•å¥è¿˜æ˜¯åŒå¥ã€‚  \n",
    "attention_mask: æ ‡è®°paddingéƒ¨åˆ†,è®©self-attentionåªå…³æ³¨épaddingéƒ¨åˆ†ã€‚  \n",
    "special_tokens_mask: æ ‡è®°special tokensçš„mask,æ¯”å¦‚[CLS]ç­‰ã€‚  \n",
    "return_length: è¿”å›è¾“å…¥çš„é•¿åº¦ã€‚  \n",
    "æ‰€ä»¥encode_plusç›¸æ¯”encode,æä¾›äº†æ›´å…¨é¢çš„ä¿¡æ¯,åŒ…æ‹¬è¾“å…¥åºåˆ—ã€è¾“å…¥é•¿åº¦ã€æ ‡è®°ä¸åŒéƒ¨åˆ†çš„maskç­‰,è¿™äº›ä¿¡æ¯åœ¨BERTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ä¸­éƒ½æ˜¯å¿…è¦çš„ã€‚  \n",
    "\n",
    "è°ƒç”¨è¿”å›çš„æ˜¯å­—å…¸ç±»å‹,åŒ…å«ä»¥ä¸Šå„ä¸ªç¼–ç ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥æ‹¿è¿™ä¸ªå­—å…¸å–‚ç»™BERTæ¨¡å‹,è€Œä¸éœ€è¦è‡ªå·±å»è®¡ç®—maskã€é•¿åº¦ç­‰ä¿¡æ¯ã€‚è¿™æ ·å¯ä»¥ç®€åŒ–ä½¿ç”¨BERTçš„æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7691f9-c4fa-4111-9f6d-325e34ffc38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]\n",
      "token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "special_tokens_mask : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "length : 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] é€‰ æ‹© ç  æ±Ÿ èŠ± å›­ çš„ åŸ å›  å°± æ˜¯ æ–¹ ä¾¿ ã€‚ [SEP] ç¬” è®° æœ¬ çš„ é”® ç›˜ ç¡® å® çˆ½ ã€‚ [SEP] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619dc325-2f38-4e77-8b3d-2912caefc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ‰¹é‡ç¼–ç ä¸€ä¸ªä¸€ä¸ªçš„å¥å­\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[sents[0], sents[1]],  # æ‰¹é‡ç¼–ç ï¼Œä¸€æ¬¡ç¼–ç äº†ä¸¤ä¸ªå¥å­(ä¸å¢å¼ºçš„ç¼–ç å‡½æ•°ç›¸æ¯”ï¼Œå°±æ­¤å¤„ä¸åŒ)\n",
    "\n",
    "    #å½“å¥å­é•¿åº¦å¤§äºmax_lengthæ—¶,æˆªæ–­\n",
    "    truncation=True,\n",
    "\n",
    "    #ä¸€å¾‹è¡¥é›¶åˆ°max_lengthé•¿åº¦\n",
    "    padding='max_length',\n",
    "    max_length=15,\n",
    "    add_special_tokens=True,\n",
    "    \n",
    "    #å¯å–å€¼tf,pt,np,é»˜è®¤ä¸ºè¿”å›list\n",
    "    return_tensors=None,\n",
    "\n",
    "    #è¿”å›token_type_ids\n",
    "    return_token_type_ids=True,\n",
    "\n",
    "    #è¿”å›attention_mask\n",
    "    return_attention_mask=True,\n",
    "\n",
    "    #è¿”å›special_tokens_mask ç‰¹æ®Šç¬¦å·æ ‡è¯†\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #è¿”å›offset_mapping æ ‡è¯†æ¯ä¸ªè¯çš„èµ·æ­¢ä½ç½®,è¿™ä¸ªå‚æ•°åªèƒ½BertTokenizerFastä½¿ç”¨\n",
    "    #return_offsets_mapping=True,\n",
    "\n",
    "    #è¿”å›length æ ‡è¯†é•¿åº¦\n",
    "    return_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d095a5-8ec4-4d46-a42d-372512c93401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 102], [101, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]\n",
      "length : [15, 12]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('[CLS] é€‰ æ‹© ç  æ±Ÿ èŠ± å›­ çš„ åŸ å›  å°± æ˜¯ æ–¹ ä¾¿ [SEP]',\n",
       " '[CLS] ç¬” è®° æœ¬ çš„ é”® ç›˜ ç¡® å® çˆ½ ã€‚ [SEP] [PAD] [PAD] [PAD]')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out['input_ids'][0]), tokenizer.decode(out['input_ids'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d94d2cc-2341-48dc-aa2b-d61bb3fa5ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "#æ‰¹é‡ç¼–ç æˆå¯¹çš„å¥å­\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[(sents[0], sents[1]), (sents[2], sents[3])],   # tuple\n",
    "\n",
    "    #å½“å¥å­é•¿åº¦å¤§äºmax_lengthæ—¶,æˆªæ–­\n",
    "    truncation=True,\n",
    "\n",
    "    #ä¸€å¾‹è¡¥é›¶åˆ°max_lengthé•¿åº¦\n",
    "    padding='max_length',\n",
    "    max_length=30,\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    #å¯å–å€¼tf,pt,np,é»˜è®¤ä¸ºè¿”å›list\n",
    "    return_tensors=None,\n",
    "\n",
    "    #è¿”å›token_type_ids\n",
    "    return_token_type_ids=True,\n",
    "\n",
    "    #è¿”å›attention_mask\n",
    "    return_attention_mask=True,\n",
    "\n",
    "    #è¿”å›special_tokens_mask ç‰¹æ®Šç¬¦å·æ ‡è¯†\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #è¿”å›offset_mapping æ ‡è¯†æ¯ä¸ªè¯çš„èµ·æ­¢ä½ç½®,è¿™ä¸ªå‚æ•°åªèƒ½BertTokenizerFastä½¿ç”¨\n",
    "    #return_offsets_mapping=True,\n",
    "\n",
    "    #è¿”å›length æ ‡è¯†é•¿åº¦\n",
    "    return_length=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2629fbb4-4a6a-4c29-a4d2-1d366e408167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0], [101, 2791, 7313, 1922, 2207, 511, 1071, 800, 4638, 6963, 671, 5663, 511, 102, 791, 1921, 2798, 4761, 6887, 6821, 741, 6820, 3300, 5018, 127, 1318, 117, 4696, 3300, 102]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "length : [27, 30]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] é€‰ æ‹© ç  æ±Ÿ èŠ± å›­ çš„ åŸ å›  å°± æ˜¯ æ–¹ ä¾¿ ã€‚ [SEP] ç¬” è®° æœ¬ çš„ é”® ç›˜ ç¡® å® çˆ½ ã€‚ [SEP] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfaa0d77-93d0-4f93-98b6-3d906dd71de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 21128, False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#è·å–å­—å…¸\n",
    "zidian = tokenizer.get_vocab()\n",
    "\n",
    "type(zidian), len(zidian), 'æœˆå…‰' in zidian,   # (dict, 21128, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f28555-f90e-42b2-a2a0-5ff861ea2fd9",
   "metadata": {},
   "source": [
    "è¿™é‡Œæˆ‘ä»¬è°ƒç”¨äº†get_vocab()æ–¹æ³•æ¥è·å–åˆ†è¯å™¨çš„è¯å…¸ã€‚è¯å…¸æ˜¯ä¸€ä¸ªå­—å…¸ç±»å‹,é”®æ˜¯è¯,å€¼æ˜¯è¯å¯¹åº”çš„æ•°å­—ç´¢å¼•ã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°:  \n",
    "\n",
    "è¯å…¸æ˜¯dictç±»å‹   \n",
    "è¯å…¸å¤§å°æ˜¯21128ä¸ªè¯  \n",
    "è¯å…¸ä¸­ä¸åŒ…å«'æœˆå…‰'è¿™ä¸ªè¯  \n",
    "è¿™éªŒè¯äº†BERTä½¿ç”¨çš„æ˜¯WordPieceç®—æ³•è¿›è¡Œåˆ†è¯,è¯å…¸ä¸­åŒ…å«å­—æ¯ã€è¯æ±‡å’Œè¯ç»„,ä½†ä¸ä¼šåŒ…å«æ‰€æœ‰æ±‰è¯­è¯æ±‡ã€‚  \n",
    "\n",
    "é€šè¿‡è·å–è¯å…¸æˆ‘ä»¬å¯ä»¥äº†è§£åˆ°åˆ†è¯å™¨çš„è¯æ±‡é‡å’Œè¯æ±‡è¦†ç›–æƒ…å†µ,ä¹Ÿå¯ä»¥ç›´æ¥æŸ¥è¯¢æŸä¸ªè¯æ˜¯å¦åœ¨è¯å…¸ä¸­ã€‚è¿™å¯¹åˆ†æBERTè¯­è¨€è¡¨ç¤ºèƒ½åŠ›ä»¥åŠä¼˜åŒ–è¯å…¸éå¸¸æœ‰å¸®åŠ©ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84edd9bd-4c3c-4384-aed7-3f10fc56a63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 21131, 21128, 21130, 21129)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#æ·»åŠ æ–°è¯\n",
    "tokenizer.add_tokens(new_tokens=['æœˆå…‰', 'å¸Œæœ›'])\n",
    "\n",
    "#æ·»åŠ æ–°ç¬¦å·\n",
    "tokenizer.add_special_tokens({'eos_token': '[EOS]'})   # End Of Sentence\n",
    "\n",
    "zidian = tokenizer.get_vocab()\n",
    "\n",
    "type(zidian), len(zidian), zidian['æœˆå…‰'], zidian['[EOS]'],zidian['å¸Œæœ›']   # (dict, 21131, 21128, 21130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb533d91-0886-4bd9-9507-fd3a96ed95c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 21128, 4638, 3173, 21129, 21130, 102, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] æœˆå…‰ çš„ æ–° å¸Œæœ› [EOS] [SEP] [PAD]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ç¼–ç æ–°æ·»åŠ çš„è¯\n",
    "out = tokenizer.encode(\n",
    "    text='æœˆå…‰çš„æ–°å¸Œæœ›[EOS]',\n",
    "    text_pair=None,\n",
    "\n",
    "    #å½“å¥å­é•¿åº¦å¤§äºmax_lengthæ—¶,æˆªæ–­\n",
    "    truncation=True,\n",
    "\n",
    "    #ä¸€å¾‹è¡¥padåˆ°max_lengthé•¿åº¦\n",
    "    padding='max_length',\n",
    "    add_special_tokens=True,\n",
    "    max_length=8,\n",
    "    \n",
    "    return_tensors=None,\n",
    ")\n",
    "\n",
    "print(out)\n",
    "\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a51025f-15d4-4368-bd00-87c83eb66d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/apple/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-eaea6a9750cb0fe7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b915d57687471bbddffd8d21241a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ChnSentiCorpæ•°æ®é›†åŒ…å«æ­£é¢å’Œè´Ÿé¢è¯„è®ºå¥å­,å¯ä»¥ç”¨æ¥è¿›è¡Œä¸­æ–‡æ–‡æœ¬çš„æƒ…æ„Ÿåˆ†æã€‚\n",
    "#https://github.com/brightmart/nlp_chinese_corpus\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "#åŠ è½½æ•°æ®\n",
    "dataset = load_dataset(path='lansinuote/ChnSentiCorp')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3be66a58-7835-4703-858a-ffa70e227252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ä¿å­˜æ•°æ®é›†åˆ°ç£ç›˜\n",
    "dataset.save_to_disk(dataset_dict_path='./data/ChnSentiCorp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da6d26af-e7d9-45be-8844-1de1a3761cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'é€‰æ‹©ç æ±ŸèŠ±å›­çš„åŸå› å°±æ˜¯æ–¹ä¾¿ï¼Œæœ‰ç”µåŠ¨æ‰¶æ¢¯ç›´æ¥åˆ°è¾¾æµ·è¾¹ï¼Œå‘¨å›´é¤é¦†ã€é£Ÿå»Šã€å•†åœºã€è¶…å¸‚ã€æ‘Šä½ä¸€åº”ä¿±å…¨ã€‚é…’åº—è£…ä¿®ä¸€èˆ¬ï¼Œä½†è¿˜ç®—æ•´æ´ã€‚ æ³³æ± åœ¨å¤§å ‚çš„å±‹é¡¶ï¼Œå› æ­¤å¾ˆå°ï¼Œä¸è¿‡å¥³å„¿å€’æ˜¯å–œæ¬¢ã€‚ åŒ…çš„æ—©é¤æ˜¯è¥¿å¼çš„ï¼Œè¿˜ç®—ä¸°å¯Œã€‚ æœåŠ¡å—ï¼Œä¸€èˆ¬',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#å–å‡ºè®­ç»ƒé›†\n",
    "dataset = dataset['train']\n",
    "#æŸ¥çœ‹ä¸€ä¸ªæ•°æ®\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "137d743f-675c-4429-a664-a0e72b0f446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j9/ryqgqp6n03schy65y0b0g4sm0000gn/T/ipykernel_42992/1035245866.py:4: FutureWarning: list_metrics is deprecated and will be removed in the next major version of datasets. Use 'evaluate.list_evaluation_modules' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metrics_list = list_metrics()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(119,\n",
       " ['accuracy',\n",
       "  'bertscore',\n",
       "  'bleu',\n",
       "  'bleurt',\n",
       "  'brier_score',\n",
       "  'cer',\n",
       "  'character',\n",
       "  'charcut_mt',\n",
       "  'chrf',\n",
       "  'code_eval',\n",
       "  'comet',\n",
       "  'competition_math',\n",
       "  'coval',\n",
       "  'cuad',\n",
       "  'exact_match',\n",
       "  'f1',\n",
       "  'frugalscore',\n",
       "  'glue',\n",
       "  'google_bleu',\n",
       "  'indic_glue',\n",
       "  'mae',\n",
       "  'mahalanobis',\n",
       "  'mape',\n",
       "  'mase',\n",
       "  'matthews_correlation',\n",
       "  'mauve',\n",
       "  'mean_iou',\n",
       "  'meteor',\n",
       "  'mse',\n",
       "  'nist_mt',\n",
       "  'pearsonr',\n",
       "  'perplexity',\n",
       "  'poseval',\n",
       "  'precision',\n",
       "  'r_squared',\n",
       "  'recall',\n",
       "  'rl_reliability',\n",
       "  'roc_auc',\n",
       "  'rouge',\n",
       "  'sacrebleu',\n",
       "  'sari',\n",
       "  'seqeval',\n",
       "  'smape',\n",
       "  'spearmanr',\n",
       "  'squad',\n",
       "  'squad_v2',\n",
       "  'super_glue',\n",
       "  'ter',\n",
       "  'trec_eval',\n",
       "  'wer',\n",
       "  'wiki_split',\n",
       "  'xnli',\n",
       "  'xtreme_s',\n",
       "  'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics',\n",
       "  'BucketHeadP65/confusion_matrix',\n",
       "  'BucketHeadP65/roc_curve',\n",
       "  'Drunper/metrica_tesi',\n",
       "  'Felipehonorato/eer',\n",
       "  'He-Xingwei/sari_metric',\n",
       "  'JP-SystemsX/nDCG',\n",
       "  'Josh98/nl2bash_m',\n",
       "  'Kyle1668/squad',\n",
       "  'Muennighoff/code_eval',\n",
       "  'NCSOFT/harim_plus',\n",
       "  'Natooz/ece',\n",
       "  'NikitaMartynov/spell-check-metric',\n",
       "  'Pipatpong/perplexity',\n",
       "  'Splend1dchan/cosine_similarity',\n",
       "  'Viona/fuzzy_reordering',\n",
       "  'Viona/kendall_tau',\n",
       "  'Vipitis/shadermatch',\n",
       "  'Yeshwant123/mcc',\n",
       "  'abdusah/aradiawer',\n",
       "  'abidlabs/mean_iou',\n",
       "  'abidlabs/mean_iou2',\n",
       "  'andstor/code_perplexity',\n",
       "  'angelina-wang/directional_bias_amplification',\n",
       "  'aryopg/roc_auc_skip_uniform_labels',\n",
       "  'brian920128/doc_retrieve_metrics',\n",
       "  'bstrai/classification_report',\n",
       "  'chanelcolgate/average_precision',\n",
       "  'ckb/unigram',\n",
       "  'codeparrot/apps_metric',\n",
       "  'cpllab/syntaxgym',\n",
       "  'dvitel/codebleu',\n",
       "  'ecody726/bertscore',\n",
       "  'fschlatt/ner_eval',\n",
       "  'giulio98/codebleu',\n",
       "  'guydav/restrictedpython_code_eval',\n",
       "  'harshhpareek/bertscore',\n",
       "  'hpi-dhc/FairEval',\n",
       "  'hynky/sklearn_proxy',\n",
       "  'hyperml/balanced_accuracy',\n",
       "  'ingyu/klue_mrc',\n",
       "  'jpxkqx/peak_signal_to_noise_ratio',\n",
       "  'jpxkqx/signal_to_reconstruction_error',\n",
       "  'k4black/codebleu',\n",
       "  'kaggle/ai4code',\n",
       "  'langdonholmes/cohen_weighted_kappa',\n",
       "  'lhy/hamming_loss',\n",
       "  'lhy/ranking_loss',\n",
       "  'lvwerra/accuracy_score',\n",
       "  'manueldeprada/beer',\n",
       "  'mfumanelli/geometric_mean',\n",
       "  'omidf/squad_precision_recall',\n",
       "  'posicube/mean_reciprocal_rank',\n",
       "  'sakusakumura/bertscore',\n",
       "  'sma2023/wil',\n",
       "  'spidyidcccc/bertscore',\n",
       "  'tialaeMceryu/unigram',\n",
       "  'transZ/sbert_cosine',\n",
       "  'transZ/test_parascore',\n",
       "  'transformersegmentation/segmentation_scores',\n",
       "  'unitxt/metric',\n",
       "  'unnati/kendall_tau_distance',\n",
       "  'weiqis/pajm',\n",
       "  'ybelkada/cocoevaluate',\n",
       "  'yonting/average_precision_score',\n",
       "  'yuyijiong/quad_match_score'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import list_metrics\n",
    "\n",
    "#åˆ—å‡ºè¯„ä»·æŒ‡æ ‡\n",
    "metrics_list = list_metrics()\n",
    "len(metrics_list), metrics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea2727-dc47-4c5b-8056-df95343a9d16",
   "metadata": {},
   "source": [
    "è¿™é‡Œä½¿ç”¨datasetsåº“çš„list_metricså‡½æ•°åˆ—å‡ºäº†æ‰€æœ‰å†…ç½®çš„è¯„ä»·æŒ‡æ ‡ã€‚è¿™æ ·å°±ä¸ç”¨è‡ªå·±æ‰¾å‡½æ•°å®šä¹‰è¯„ä»·æŒ‡æ ‡æˆ–è€…è®¡ç®—è¯„ä»·æŒ‡æ ‡äº†ã€‚  \n",
    "ä½¿ç”¨list_metricså¯ä»¥å¿«é€ŸæŸ¥çœ‹datasetså½“å‰æ”¯æŒçš„æ‰€æœ‰è¯„ä»·æŒ‡æ ‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebaae2d1-0d96-4535-95ca-c8e9c67bb0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j9/ryqgqp6n03schy65y0b0g4sm0000gn/T/ipykernel_42992/1543994463.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', 'mrpc')   # MRPC(The Microsoft Research Paraphrase Corpusï¼Œå¾®è½¯ç ”ç©¶é™¢é‡Šä¹‰è¯­æ–™åº“)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
      "Args:\n",
      "    predictions: list of predictions to score.\n",
      "        Each translation should be tokenized into a list of tokens.\n",
      "    references: list of lists of references for each translation.\n",
      "        Each reference should be tokenized into a list of tokens.\n",
      "Returns: depending on the GLUE subset, one or several of:\n",
      "    \"accuracy\": Accuracy\n",
      "    \"f1\": F1 score\n",
      "    \"pearson\": Pearson Correlation\n",
      "    \"spearmanr\": Spearman Correlation\n",
      "    \"matthews_correlation\": Matthew Correlation\n",
      "Examples:\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0, 'f1': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
      "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
      "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'matthews_correlation': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "#åŠ è½½ä¸€ä¸ªè¯„ä»·æŒ‡æ ‡\n",
    "metric = load_metric('glue', 'mrpc')   # MRPC(The Microsoft Research Paraphrase Corpusï¼Œå¾®è½¯ç ”ç©¶é™¢é‡Šä¹‰è¯­æ–™åº“)\n",
    "print(metric.inputs_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fdfcdc1-a07d-4292-adbd-c3e8761a4738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666, 'f1': 0.6666666666666666}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#è®¡ç®—ä¸€ä¸ªè¯„ä»·æŒ‡æ ‡\n",
    "predictions = [0, 1, 0]\n",
    "references = [0, 1, 1]\n",
    "\n",
    "final_score = metric.compute(predictions=predictions, references=references)\n",
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "120510f9-19fc-4411-9b13-a2ab13a5ffa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/apple/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-eaea6a9750cb0fe7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9600,\n",
       " ('é€‰æ‹©ç æ±ŸèŠ±å›­çš„åŸå› å°±æ˜¯æ–¹ä¾¿ï¼Œæœ‰ç”µåŠ¨æ‰¶æ¢¯ç›´æ¥åˆ°è¾¾æµ·è¾¹ï¼Œå‘¨å›´é¤é¦†ã€é£Ÿå»Šã€å•†åœºã€è¶…å¸‚ã€æ‘Šä½ä¸€åº”ä¿±å…¨ã€‚é…’åº—è£…ä¿®ä¸€èˆ¬ï¼Œä½†è¿˜ç®—æ•´æ´ã€‚ æ³³æ± åœ¨å¤§å ‚çš„å±‹é¡¶ï¼Œå› æ­¤å¾ˆå°ï¼Œä¸è¿‡å¥³å„¿å€’æ˜¯å–œæ¬¢ã€‚ åŒ…çš„æ—©é¤æ˜¯è¥¿å¼çš„ï¼Œè¿˜ç®—ä¸°å¯Œã€‚ æœåŠ¡å—ï¼Œä¸€èˆ¬',\n",
       "  1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "#å®šä¹‰æ•°æ®é›†\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        # åŠ è½½æ•°æ®ã€‚ChnSentiCorpä¸ºæ¶ˆè´¹è¯„ä»·æ•°æ®é›†ï¼Œåˆ†å¥½è¯„å’Œå·®è¯„\n",
    "        self.dataset = load_dataset(path='lansinuote/ChnSentiCorp', split=split)  # Available splits: ['test', 'train', 'validation']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "        return text, label\n",
    "\n",
    "dataset = Dataset('train')\n",
    "len(dataset), dataset[0]  # è®­ç»ƒé›†æœ‰9600å¥è¯ï¼›dataset[0]è¡¨ç¤ºç¬¬1å¥è¯ï¼Œå‰é¢æ˜¯textï¼Œåé¢æ˜¯labelï¼Œ1è¡¨ç¤ºå·®è¯„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7a1e7fa-5154-467a-a6fa-7e04a32a0408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#åŠ è½½å­—å…¸å’Œåˆ†è¯å·¥å…·ï¼Œå³tokenizer\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')  # è¦è·Ÿé¢„è®­ç»ƒæ¨¡å‹ç›¸åŒ¹é…\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b0c16f4-e92b-4bc5-98b8-3654bb5b2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ‰¹å¤„ç†å‡½æ•°\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]#æå–æ–‡æœ¬\n",
    "    labels = [i[1] for i in data]#æå–æ ‡ç­¾\n",
    "\n",
    "    #ç¼–ç \n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,   # å½“å¥å­é•¿åº¦å¤§äºmax_lengthæ—¶ï¼Œæˆªæ–­\n",
    "                                   padding='max_length',   # ä¸€å¾‹è¡¥0åˆ°max_lengthé•¿åº¦\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt',   # è¿”å›pytorchç±»å‹çš„tensor\n",
    "                                   return_length=True)   # è¿”å›lengthï¼Œæ ‡è¯†é•¿åº¦\n",
    "\n",
    "    input_ids = data['input_ids']    # input_ids:ç¼–ç ä¹‹åçš„æ•°å­—\n",
    "    attention_mask = data['attention_mask']     # attention_mask:è¡¥é›¶çš„ä½ç½®æ˜¯0,å…¶ä»–ä½ç½®æ˜¯1\n",
    "    token_type_ids = data['token_type_ids']   # ç¬¬ä¸€ä¸ªå¥å­å’Œç‰¹æ®Šç¬¦å·çš„ä½ç½®æ˜¯0ï¼Œç¬¬äºŒä¸ªå¥å­çš„ä½ç½®æ˜¯1(åŒ…æ‹¬ç¬¬äºŒä¸ªå¥å­åçš„[SEP])\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c53e583-53ce-4597-87c6-a5be909b7c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#æ•°æ®åŠ è½½å™¨\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "# æŸ¥çœ‹æ•°æ®æ ·ä¾‹\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))  # 600 = 9600/16\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels   # 500è¡¨ç¤ºå¥å­æœ€å¤§é•¿åº¦ä¸º500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2320d422-222d-4501-af80-bc72af5f9880",
   "metadata": {},
   "source": [
    "å®šä¹‰äº†ä¸€ä¸ªDataLoader,å‚æ•°åŒ…æ‹¬:  \n",
    "dataset:è¦åŠ è½½çš„æ•°æ®é›†  \n",
    "batch_size:æ‰¹å¤§å°ä¸º16  \n",
    "collate_fn:ä½¿ç”¨ä¹‹å‰å®šä¹‰å¥½çš„collateå‡½æ•°è¿›è¡Œæ‰¹å¤„ç†  \n",
    "shuffle:æ‰“ä¹±æ•°æ®  \n",
    "drop_last:å»æ‰æœ€åä¸å®Œæ•´çš„batch  \n",
    "éå†loader,æŸ¥çœ‹ä¸€ä¸ªbatchçš„æ•°æ®æ ·ä¾‹  \n",
    "\n",
    "æ‰“å°loaderçš„é•¿åº¦,ä»¥åŠä¸€ä¸ªbatchä¸­å„ä¸ªtensorçš„shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd5e18ea-ad51-4d29-a4ef-e582f3eefb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "print(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "386c185e-4ad5-4cb5-9e1f-d93d37a18cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¸è®­ç»ƒ,ä¸éœ€è¦è®¡ç®—æ¢¯åº¦\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)  # è¿™é‡Œä¸ä½¿ç”¨fine-tuningï¼Œç›´æ¥æŠŠé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°å†»ç»“ä½ï¼Œåªè®­ç»ƒä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹ï¼Œå¯¹é¢„è®­ç»ƒæ¨¡å‹æœ¬èº«çš„å‚æ•°ä¸è°ƒæ•´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694369a-35af-47eb-8a23-42c99a35ba20",
   "metadata": {},
   "source": [
    "è¿™æ®µä»£ç çš„ç›®çš„æ˜¯å†»ç»“é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(å¦‚BERT)çš„å‚æ•°,ä½¿å…¶åœ¨è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡æ—¶ä¸æ›´æ–°ã€‚\n",
    "\n",
    "ä¸»è¦æ­¥éª¤æ˜¯:\n",
    "\n",
    "éå†é¢„è®­ç»ƒæ¨¡å‹pretrainedä¸­çš„æ‰€æœ‰å‚æ•°paramã€‚\n",
    "\n",
    "å¯¹æ¯ä¸ªå‚æ•°è°ƒç”¨requires_grad_(False),å°†requires_gradæ ‡å¿—ç½®ä¸ºFalseã€‚\n",
    "\n",
    "è¿™æ ·åœ¨è®­ç»ƒæ—¶è®¡ç®—å›¾å°±ä¸ä¼šè·Ÿè¸ªè¿™äº›å‚æ•°çš„æ¢¯åº¦,å®ç°äº†å†»ç»“é¢„è®­ç»ƒå‚æ•°çš„æ•ˆæœã€‚\n",
    "\n",
    "è¿™ä¸ªæŠ€å·§é€šå¸¸ç”¨åœ¨ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åšä¸‹æ¸¸ä»»åŠ¡çš„æ—¶å€™,å…ˆå†»ç»“é¢„è®­ç»ƒæ¨¡å‹,åªè®­ç»ƒæ–°å¢çš„å‚æ•°,ç„¶åå†è§£å†»åæ•´ä½“å¾®è°ƒã€‚\n",
    "\n",
    "å®ƒå¯ä»¥é˜²æ­¢é¢„è®­ç»ƒå‚æ•°åœ¨ä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒåˆæœŸè¢«ç ´å,å¹¶ä¸”åŠ é€Ÿæ”¶æ•›ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05d54320-d7cb-4fc4-8aa6-cda110ed2917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 500, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#æ¨¡å‹è¯•ç®—\n",
    "out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "\n",
    "out.last_hidden_state.shape   # [batch_size, æ•°æ®åˆ†è¯çš„é•¿åº¦(æ¯ä¸€å¥è¯ç¼–ç æˆ500ä¸ªè¯çš„é•¿åº¦), è¯ç¼–ç çš„ç»´åº¦(å³æ¯ä¸€ä¸ªè¯ç¼–ç æˆä¸€ä¸ª768ç»´çš„å‘é‡)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9e1be-61b3-4df3-801d-51308394559e",
   "metadata": {},
   "source": [
    "è¿™é‡Œä½¿ç”¨äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(å¦‚BERT)å¯¹è¾“å…¥è¿›è¡Œäº†å‰å‘è®¡ç®—,å¹¶æ‰“å°å‡ºè¾“å‡ºtensorçš„shapeã€‚\n",
    "\n",
    "ä¸»è¦çš„æ­¥éª¤æ˜¯:  \n",
    "å°†ç»è¿‡åˆ†è¯å’Œç¼–ç åçš„inputä½œä¸ºè¾“å…¥,åŒ…æ‹¬input_ids,attention_mask,token_type_idsã€‚  \n",
    "æŠŠè¿™äº›è¾“å…¥ä¼ ç»™é¢„è®­ç»ƒæ¨¡å‹pretrained,è°ƒç”¨æ¨¡å‹æ‰§è¡Œå‰å‘è®¡ç®—ã€‚  \n",
    "ä»è¾“å‡ºoutä¸­å–å‡ºæœ€åä¸€å±‚çš„éšçŠ¶æ€last_hidden_stateã€‚  \n",
    "æ‰“å°å‡ºlast_hidden_stateçš„shapeã€‚  \n",
    "\n",
    "ä»æ‰“å°çš„shapeå¯ä»¥çœ‹å‡º:  \n",
    "ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯batch size,å³ä¸€æ¬¡è¾“å…¥çš„æ ·æœ¬æ•°é‡ã€‚  \n",
    "ç¬¬äºŒä¸ªç»´åº¦æ˜¯è¾“å…¥åºåˆ—é•¿åº¦,ç»è¿‡åˆ†è¯å’Œtruncation/paddingåéƒ½å¤„ç†æˆäº†å›ºå®šé•¿åº¦ã€‚  \n",
    "ç¬¬ä¸‰ä¸ªç»´åº¦æ˜¯è¯å‘é‡ç»´åº¦,å³BERTæ¨¡å‹å¯¹æ¯ä¸ªè¯ç¼–ç æˆçš„å‘é‡è¡¨ç¤ºç»´åº¦ã€‚  \n",
    "æ‰€ä»¥è¾“å‡ºtensorçš„å½¢çŠ¶åæ˜ äº†æ¨¡å‹å¤„ç†æ‰¹é‡åºåˆ—è¾“å…¥çš„ç»“æœã€‚\n",
    "\n",
    "æ¯æ¬¡å‘BERTç­‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æä¾›è¾“å…¥æ—¶,éƒ½éœ€è¦æŒ‰ç…§è¿™ä¸ªæ ¼å¼æä¾›ã€‚  \n",
    "å…·ä½“æ¥è¯´,è¾“å…¥å¯¹åº”çš„æ¨¡å‹éƒ¨åˆ†æ˜¯:  \n",
    "input_ids:å¯¹åº”embeddingå±‚çš„è¾“å…¥,è¡¨ç¤ºå•è¯çš„ç´¢å¼•ä¿¡æ¯ã€‚  \n",
    "attention_mask:å¯¹åº”self-attentionå±‚çš„maskè¾“å…¥,æŒ‡ç¤ºå“ªäº›è¯æ˜¯padding,å“ªäº›è¯æ˜¯æœ‰æ•ˆè¯ã€‚  \n",
    "token_type_ids:å¯¹åº”ç¬¬ä¸€å¥å’Œç¬¬äºŒå¥çš„åŒºåˆ†,è®©æ¨¡å‹å­¦ä¹ å¥å­å…³ç³»ã€‚  \n",
    "è¿™ä¸‰ä¸ªè¾“å…¥åˆ†åˆ«æä¾›äº†è¾“å…¥åºåˆ—ã€æ ‡è®°å’Œå¥å­å…³ç³»ä¿¡æ¯ã€‚å®ƒä»¬å…±åŒå®šä¹‰äº†æ¨¡å‹çš„æ–‡æœ¬è¾“å…¥ã€‚\n",
    "\n",
    "æ¨¡å‹å†…éƒ¨çš„è®¡ç®—æµç¨‹æ˜¯:  \n",
    "è¾“å…¥å±‚æ ¹æ®input_idsæŸ¥è¡¨è·å–è¯å‘é‡è¡¨ç¤º;  \n",
    "å¤šå±‚Transformerç¼–ç å™¨ç»“æ„æ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯;  \n",
    "æœ€ç»ˆè¾“å‡ºåŒ…å«å¥å­çº§è¯­ä¹‰ä¿¡æ¯çš„è¯å‘é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "121b6f95-d517-4c42-98b1-d085df0ba534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#å®šä¹‰ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)  # å•å±‚ç½‘ç»œæ¨¡å‹ï¼ŒåªåŒ…æ‹¬äº†ä¸€ä¸ªfcçš„ç¥ç»ç½‘ç»œ\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,   # å…ˆæ‹¿é¢„è®­ç»ƒæ¨¡å‹æ¥åšä¸€ä¸ªè®¡ç®—ï¼ŒæŠ½å–æ•°æ®å½“ä¸­çš„ç‰¹å¾\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "\n",
    "        # æŠŠæŠ½å–å‡ºæ¥çš„ç‰¹å¾æ”¾åˆ°å…¨è¿æ¥ç½‘ç»œä¸­è¿ç®—ï¼Œä¸”ç‰¹å¾çš„ç»“æœåªéœ€è¦ç¬¬0ä¸ªè¯çš„ç‰¹å¾(è·Ÿbertæ¨¡å‹çš„è®¾è®¡æ–¹å¼æœ‰å…³ã€‚å¯¹å¥å­çš„æƒ…æ„Ÿåˆ†ç±»ï¼Œåªéœ€è¦æ‹¿ç‰¹å¾ä¸­çš„ç¬¬0ä¸ªè¯æ¥è¿›è¡Œåˆ†ç±»å°±å¯ä»¥äº†)\n",
    "        out = self.fc(out.last_hidden_state[:, 0])   # torch.Size([16, 768])\n",
    "        \n",
    "        # å°†softmaxå‡½æ•°åº”ç”¨äºä¸€ä¸ªnç»´è¾“å…¥å¼ é‡ï¼Œå¯¹å…¶è¿›è¡Œç¼©æ”¾ï¼Œä½¿nç»´è¾“å‡ºå¼ é‡çš„å…ƒç´ ä½äº[0,1]èŒƒå›´å†…ï¼Œæ€»å’Œä¸º1\n",
    "        out = out.softmax(dim=1)  \n",
    "\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape    # torch.Size([16, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff41425-dd50-4d06-a56f-8397232ccc12",
   "metadata": {},
   "source": [
    "è¿™ä¸ªå®šä¹‰çš„è¿™ä¸ªä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹å®ç°äº†åŸºäºBERTçš„æ–‡æœ¬åˆ†ç±»,ä¸»è¦æ­¥éª¤å¦‚ä¸‹:\n",
    "å®šä¹‰Modelç±»,ç»§æ‰¿torch.nn.Moduleã€‚  \n",
    "åˆå§‹åŒ–æ—¶å®šä¹‰äº†ä¸€ä¸ªçº¿æ€§å±‚fc,è¾“å…¥æ˜¯BERTè¾“å‡ºç»´åº¦768,è¾“å‡ºæ˜¯2åˆ†ç±»ã€‚  \n",
    "forwardå‡½æ•°ä¸­,é¦–å…ˆä½¿ç”¨torch.no_grad()ä¸è®¡ç®—æ¢¯åº¦åœ°è¿è¡ŒBERTè·å–ç‰¹å¾ã€‚  \n",
    "ç„¶åå–æœ€åä¸€å±‚éšçŠ¶æ€çš„ç¬¬ä¸€ä¸ªtokenä½œä¸ºåˆ†ç±»ç‰¹å¾,è¾“å…¥åˆ°fcå±‚ã€‚  \n",
    "å¯¹fcå±‚è¾“å‡ºä½¿ç”¨softmaxè·å¾—å½’ä¸€åŒ–çš„é¢„æµ‹åˆ†å¸ƒã€‚  \n",
    "è¿™æ ·å³å®ç°äº†åˆ©ç”¨BERTæå–ç‰¹å¾,ç„¶åæ¥ä¸€ä¸ªç®€å•çš„çº¿æ€§åˆ†ç±»å™¨æ¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚\n",
    "\n",
    "å–æœ€åä¸€å±‚ç¬¬ä¸€ä¸ªtokenä½œä¸ºå¥å­è¡¨ç¤ºæ˜¯BERTç‰¹æœ‰çš„åšæ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17c858b2-645a-43a0-96a1-581bb8e41191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7090100049972534 0.375\n",
      "5 0.6451802253723145 0.75\n",
      "10 0.6374646425247192 0.6875\n",
      "15 0.6967511773109436 0.5625\n",
      "20 0.6140915155410767 0.75\n",
      "25 0.5840007066726685 0.75\n",
      "30 0.5491213202476501 0.9375\n",
      "35 0.6162095665931702 0.75\n",
      "40 0.5880313515663147 0.625\n",
      "45 0.5858539938926697 0.75\n",
      "50 0.5374869704246521 0.875\n",
      "55 0.5708539485931396 0.75\n",
      "60 0.533920407295227 0.8125\n",
      "65 0.5271445512771606 0.875\n",
      "70 0.49780941009521484 0.875\n",
      "75 0.5622323155403137 0.875\n",
      "80 0.5095774531364441 0.875\n",
      "85 0.47063907980918884 0.875\n",
      "90 0.4942939281463623 0.8125\n",
      "95 0.4704240560531616 0.9375\n",
      "100 0.4980355203151703 0.8125\n",
      "105 0.4693685472011566 0.9375\n",
      "110 0.4152890145778656 1.0\n",
      "115 0.4286099970340729 0.9375\n",
      "120 0.4034077227115631 1.0\n",
      "125 0.44301509857177734 1.0\n",
      "130 0.5175010561943054 0.75\n",
      "135 0.4954994022846222 0.875\n",
      "140 0.45595037937164307 0.9375\n",
      "145 0.4319436550140381 0.9375\n",
      "150 0.5459808111190796 0.875\n",
      "155 0.47220784425735474 0.875\n",
      "160 0.3745757043361664 0.9375\n",
      "165 0.48836103081703186 0.8125\n",
      "170 0.5114022493362427 0.875\n",
      "175 0.3824004530906677 1.0\n",
      "180 0.4282243847846985 0.875\n",
      "185 0.5618668794631958 0.6875\n",
      "190 0.42371630668640137 0.9375\n",
      "195 0.4668157696723938 0.875\n",
      "200 0.44685202836990356 0.9375\n",
      "205 0.4461783468723297 0.875\n",
      "210 0.44988885521888733 0.875\n",
      "215 0.42966315150260925 0.9375\n",
      "220 0.4938286542892456 0.8125\n",
      "225 0.4855901598930359 0.875\n",
      "230 0.46268633008003235 0.875\n",
      "235 0.47675976157188416 0.875\n",
      "240 0.4031706750392914 0.9375\n",
      "245 0.5391350388526917 0.75\n",
      "250 0.48207417130470276 0.875\n",
      "255 0.4860449433326721 0.8125\n",
      "260 0.3749210238456726 1.0\n",
      "265 0.5656070113182068 0.8125\n",
      "270 0.4984758794307709 0.875\n",
      "275 0.4937138855457306 0.875\n",
      "280 0.5697503685951233 0.75\n",
      "285 0.40729373693466187 0.9375\n",
      "290 0.49484023451805115 0.8125\n",
      "295 0.5590804219245911 0.8125\n",
      "300 0.37672945857048035 0.9375\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW   # ä¼˜åŒ–å™¨ï¼Œå³ Adam + Weight decay(è‡ªé€‚åº”æ¢¯åº¦æ–¹æ³•)\n",
    "\n",
    "#è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Pytorchè®¡ç®—äº¤å‰ç†µè¯¯å·®çš„å‡½æ•°è‡ªå¸¦softmaxï¼Œæ•…è®­ç»ƒæ—¶æ¨¡å‹é‡Œä¸è¦æ·»åŠ softmax\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "    loss = criterion(out, labels)  # è¾“å‡ºè·ŸçœŸå®çš„labelsè®¡ç®—loss\n",
    "    loss.backward()   # è°ƒç”¨åå‘ä¼ æ’­å¾—åˆ°æ¯ä¸ªè¦æ›´æ–°å‚æ•°çš„æ¢¯åº¦\n",
    "    optimizer.step()  # æ¯ä¸ªå‚æ•°æ ¹æ®ä¸Šä¸€æ­¥å¾—åˆ°çš„æ¢¯åº¦è¿›è¡Œä¼˜åŒ–\n",
    "    optimizer.zero_grad()  # æŠŠä¸Šä¸€æ­¥è®­ç»ƒçš„æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦æ¸…é›¶\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "        print(i, loss.item(), accuracy)\n",
    "        if i == 300:  # åªè®­ç»ƒ300ä¸ªè½®æ¬¡ï¼Œæ²¡æœ‰æŠŠå…¨é‡çš„æ•°æ®(len(loader)= 600 = 9600/16)å…¨éƒ¨è®­ç»ƒä¸€é\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c765ed69-7d70-4e8a-b42e-fcf3bdd9320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/apple/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-eaea6a9750cb0fe7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.85625\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        if i == 5:\n",
    "            break   # åªæµ‹è¯•å‰5ä¸ª\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)  # æµ‹è¯•é›†çš„æ­£ç¡®ç‡\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ace914-f2d1-474f-84f7-ec4bdd54ad44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
